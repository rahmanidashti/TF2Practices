{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - TF2Practice - Recurrent Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1uzZ1pkhz+YSeWjdYmDa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahmanidashti/TF2Practices/blob/main/04_TF2Practice_Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XEIGmAK0Og4"
      },
      "source": [
        "# Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI0l9A5FNxby"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXS6YS8n0T7L"
      },
      "source": [
        "## Data\n",
        "*Large Movie Review Dataset*. A dataset for binary sentiment classification containing a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. The reviews are preprocessed and each one is encoded as a sequence of word indexes in the form of integers. The words within the reviews are indexed by their overall frequency within the dataset. For example, the integer “2” encodes the second most frequent word in the data. More information is available at https://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0btoE6_YN80L"
      },
      "source": [
        "def load_data():\n",
        "\n",
        "  ''' The 'num_words' argument is the number of distinct words which can be load in the training set \n",
        "  (This is the size of the vocabulary in the text data).\n",
        "  '''\n",
        "\n",
        "  (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n",
        "  '''\n",
        "  pad_sequences method adds the value (default 0) to the sequences based on max length or the longest sequence in the list.\n",
        "  The default padding is 'pre' which means it adds the values from the beginning of the sentences.\n",
        "  '''\n",
        "\n",
        "  train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=100)\n",
        "  test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=100)\n",
        "\n",
        "  return train_data, train_labels, test_data, test_labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol81vIxHxdEv"
      },
      "source": [
        "## Convert Int to Text\n",
        "\n",
        "The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. Credit: [How to build a neural network with Keras using the IMDB dataset](https://builtin.com/data-science/how-build-neural-network-keras?__cf_chl_captcha_tk__=c261bf91c002510c062f35f082f89294909a4d83-1616052406-0-AQEle5WRINPJo5B6eRDYKBxPUsGB-4HAr6CQzaC8IGqRhbuO3FcMHk8hUWVZBivvGBxVInEzD-B-QJt_dSB0vOekLt5pWOfnF3tQYCRZ7R1v0bgDzSlX7N7iLDCtdw1gglbiUc2lgu-2PnVXk_RjGE1_9gSldL2_OOF7JFR3zSmDeXYsC45se1-A65GrtIV9SuBS0KSI-GymxZ-scLye2ooHpKVycqVTkhzc7-mkk4SQRsEPw33WnV62m1uApEeuWuMUE4z8BtwpHVpA8WDkZW0gKIU0bjmYR7zk4RKHaRg_EMHrycHzq32BkM784v3VZldAtq9qJa3WMf6el3bBZ6oncZHBvG-sUNeWi60jNrvg3EaeCLJlfSPT2cDuIfxO2INFchU-nAFQdqELDNYFmZn7lkI4ExQyF5VlqRfg0nDHnrcsmUYOeg8x6vBxRiegmSlAxmF6zT4_GDNuw07sPBK1eaLPz6iLVaCXejmXx2C2c7rnmoX5RPFgoMMilQVN97VjXFJenui3UbNlnYO-g8R1s01RKXKbwJrWYuz9x_IAMOdHoii_ab0nuyoJzIcg4hlye1z3jYpD5BCgq_jOkVDCw9FfFpwbSmp2QCfRSZJOTDH5IN-MXEqgd-aZgdh_X12GUPsSQcRNDrfTDhY6jVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMGuU_3OSS0",
        "outputId": "d134939e-bb55-47e8-8c6b-a0d2e5f376d5"
      },
      "source": [
        "# See an actual review in words\n",
        "# Reverse from integers to words using the DICTIONARY (given by keras...need to do nothing to create it)\n",
        "\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "train_data, _, _, _ = load_data()\n",
        "\n",
        "reverse_word_index = dict(\n",
        "[(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "decoded_review = ' '.join(\n",
        "[reverse_word_index.get(i - 3, '?') for i in train_data[123]])\n",
        "\n",
        "print(decoded_review)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? beautiful and touching movie rich colors great settings good acting and one of the most charming movies i have seen in a while i never saw such an interesting setting when i was in china my wife liked it so much she asked me to ? on and rate it so other would enjoy too\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt9k-Bg66LDT"
      },
      "source": [
        "## Model\n",
        "The input layer is an embedding layer that embeds each word (the words are represented by an integer) into a vector. Therefore, the `input_dim` is equal to `num_words` which is defined in the `load_data` function. The `num_words` indicates the number of words which can be loaded based on the reviews. It considers the top `num_words` most frequent words. On the tasks of the model is to learn these embedding to model better the representation of the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAnGdRZuygVg"
      },
      "source": [
        "def define_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  '''\n",
        "  The first argument (10000) is the number of distinct words in the training set \n",
        "  and the second argument (128) indicates the size of the embedding vectors.\n",
        "  '''\n",
        "\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=128))\n",
        "\n",
        "  '''\n",
        "  A Long Short-Term Memory network or LSTM is a type of recurrent neural network\n",
        "  (RNN) that was developed to resolve the vanishing gradients problem. This \n",
        "  problem, which is caused by the chaining of gradients during error backpropagation,\n",
        "  means that the most upstream layers in a neural network learn very slowly.\n",
        "  '''\n",
        "\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1dKQbUg6Mfw"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H5QP9odVz04",
        "outputId": "264af78d-c985-42e0-8caa-b76ae654fcab"
      },
      "source": [
        "train_data, train_labels, test_data, test_labels = load_data()\n",
        "\n",
        "model = define_model()\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_data, train_labels, batch_size=128, epochs=5)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 11s 35ms/step - loss: 0.6769 - accuracy: 0.5568\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 7s 36ms/step - loss: 0.3616 - accuracy: 0.8482\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 7s 35ms/step - loss: 0.2620 - accuracy: 0.8998\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 7s 34ms/step - loss: 0.2151 - accuracy: 0.9238\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 7s 35ms/step - loss: 0.1951 - accuracy: 0.9339\n",
            "782/782 [==============================] - 8s 8ms/step - loss: 0.4243 - accuracy: 0.8363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCd7jo3I3xDn",
        "outputId": "bcf25782-84ae-4b79-fe88-2ba0f0e73a48"
      },
      "source": [
        "print(\"Test accuracy: %.2f\" % (test_acc * 100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 83.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63WwNHCyjCbg",
        "outputId": "677c5f0a-5f88-482a-ea3a-80d9aad93479"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 128)         1280000   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 64)          41216     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,387,393\n",
            "Trainable params: 1,387,393\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxPEMYVChmbh"
      },
      "source": [
        "## More Study\n",
        "[tf.keras.datasets.imdb.load_data](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)\n",
        "\n",
        "[How to Use Word Embedding Layers for Deep Learning with Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        "\n",
        "[How does Keras 'Embedding' layer work?](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)\n",
        "\n",
        "[Bidirectional LSTMs with TensorFlow 2.0 and Keras](https://www.machinecurve.com/index.php/2021/01/11/bidirectional-lstms-with-tensorflow-and-keras/)"
      ]
    }
  ]
}